{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw8.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILL IN YOUR NAME AND THE NAME OF YOUR PEER (IF ANY) BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Vahid Danesh\n",
    "\n",
    "**Peer**: Claude 3.7 Sonnet\n",
    "\n",
    "## Collaboration policy\n",
    "Students are responsible for writing their own quizzes, assignments, and exams. For homework assignments, students are welcome (and encouraged) to discuss problems with one peer, **but each student must write their own assignment wrtieup and code individually**. The peer must be listed at the top of the writeup for each assignment. *Note: I will treat AI assistants as peers. That is, students are welcome to discuss problems with an AI assistant, but it is considered cheating to directly obtain an answer by querying the assistant. Please credit any AI assistant that you use.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8 -- Policy gradient learning (100 pts)\n",
    "\n",
    "**Due:** Tuesday, April 8th, 2025 at 11:59 pm\n",
    "\n",
    "*HW credit: This homework is heavily based on UC Berkeley's CS294-112 HW2 from Fall '18.*\n",
    "\n",
    "This homework builds on the material in the slides and Sutton & Barto Chapter Chapter 13.\n",
    "\n",
    "We will use Jupyter/Colab notebooks throughout the semester for writing code and generating assignment outputs.\n",
    "\n",
    "**This homework will be unlike prior homeworks. It will be _entirely_ implementation-based. Some questions will be assessed by running your code, while others will require you to upload trained neural nets, which the grader will evaluate.**\n",
    "\n",
    "## 1) Implementing REINFORCE\n",
    "\n",
    "Recall that the reinforcement learning objective is to learn a $W*$ that maximizes the objective function:\n",
    "$$J(W) = \\mathbb{E}\\left[\\sum_{t}\\gamma^t R(S_t, A_t)\\right]\\enspace.$$\n",
    "\n",
    "The policy gradient approach is to directly take the gradient of this objective:\n",
    "$$ \\nabla J(\\theta) \\propto \\mathbb{E}\\left[G_t \\frac{\\nabla \\pi_W(A_t \\mid S_t)}{\\pi_W(A_t \\mid S_t)}\\right]\\enspace,$$\n",
    "\n",
    "where $G_t=\\sum_{k=t}^T \\gamma^{k-t}R(S_t, A_t)$.\n",
    "\n",
    "In practice, the expectation can be approximated from a batch of $N$ sampled trajectories:\n",
    "$$\n",
    "    \\nabla J(W) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} G_{t,i} \\frac{\\nabla\\pi_W(A_{t,i} \\mid S_{t,i})}{\\pi_W(A_{t,i} \\mid S_{t,i})}\\enspace.\n",
    "$$\n",
    "\n",
    "Here we see that the policy $\\pi_W$ is a probability distribution over the action space, conditioned on the state. In the agent-environment loop, the agent sampled an action $A_t$ from $\\pi_W(\\cdot \\mid S_t)$, and the environment responds with a reward $R(S_t, A_t)$. \n",
    "\n",
    "We saw in the lecture that subtracting a baseline that (possibly) depends on the state $S_t$ but not on the action $A_t$ does not change the expectation of the gradient, so we can alternatively compute:\n",
    "$$\n",
    "    \\nabla J(W) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} (G_{t,i}-b(S_{t,i}))\\frac{\\nabla\\pi_W(A_{t,i} \\mid S_{t,i})}{\\pi_W(A_{t,i} \\mid S_{t,i})}\\enspace.\n",
    "$$\n",
    "\n",
    "In this assignment, we will implement a value function $V_\\theta^\\pi$ that acts as a state-dependent baseline. The value function is trained to approximate the sum of future rewards starting from a particular state:\n",
    "$$\n",
    "    V_\\theta^\\pi(S_t) \\approx \\sum_{k=t}^T \\mathbb{E}[\\gamma ^{k-t}R(S_k, A_k)]\n",
    "$$\n",
    "\n",
    "\n",
    "In this problem, we will implement the REINFORCE algorithm both with and without a baseline, as well as try a few other techniques to reduce the variance of our gradient estimates (and therefore improve the quality of policy gradient training). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.1) Gaussian policy\n",
    "\n",
    "Your first task is to implement a Gaussian policy class. The class constructor will take as input the following arguments:\n",
    "- `obs_dim`: the number of features in the state\n",
    "- `action_dim`: the dimensionality of the action space\n",
    "- `hid_size`: the number of neurons in the hidden layers\n",
    "- `num_layers`: the number of hidden layers in your network. (*Note: in NN-speak, the \"input\" layer is the actual features, the \"output\" layer is the final set of nodes, and the \"hidden\" layres are all the ones in between.*)\n",
    "\n",
    "The policy will use a neural net based on the arguments described above to compute the mean $\\mu(S_t)$ of a Gaussian, and will use a trainable parameter $\\log\\sigma$ to represent the log of the standard deviation of each dimension of the action space. \n",
    "\n",
    "You can use your `FCNN` function from HW 7 to create your $\\mu$ network **but this time around we will use `nn.Tanh` as the activation function**. You should use `nn.Parameter` to define your trainable (but not state-dependent) log-standard-dev. \n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "otter": {
     "tests": [
      "q1.1"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class FCNN_11(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hid_size, num_layers):\n",
    "        super().__init__()\n",
    "        layers = nn.ModuleList()\n",
    "        current_dim = input_dim\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(current_dim, hid_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            current_dim = hid_size\n",
    "\n",
    "        layers.append(nn.Linear(hid_size, output_dim))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "class GaussianPolicy_11(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hid_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "        current_dim = obs_dim\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(current_dim, hid_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            current_dim = hid_size\n",
    "\n",
    "        layers.append(nn.Linear(hid_size, action_dim))\n",
    "\n",
    "        self.mu = nn.Sequential(*layers)\n",
    "        # Initialize the log std to all zeros\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        ''' \n",
    "        obs: Tensor of shape (batch_size, obs_dim)\n",
    "        return: mu: Tensor of shape (batch_size, action_dim)\n",
    "                std: Tensor of shape (action_dim,)\n",
    "        '''\n",
    "        mu = self.mu(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mu, std\n",
    "\n",
    "    def sample(self, obs):\n",
    "        '''\n",
    "        obs: Tensor of shape (batch_size, obs_dim)\n",
    "        return: action: Tensor of shape (batch_size, action_dim)\n",
    "\n",
    "        Hint: Look into torch.distributions.Normal\n",
    "        '''\n",
    "        mu, std = self(obs)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "        return action\n",
    "    \n",
    "    def log_prob(self, obs, action):\n",
    "        '''\n",
    "        obs: Tensor of shape (batch_size, obs_dim)\n",
    "        action: Tensor of shape (batch_size, action_dim)\n",
    "        return: log_prob: Tensor of shape (batch_size,)\n",
    "\n",
    "        Hint 1: You may use torch.distributions.Normal.log_prob\n",
    "        Hint 2: Think about how the joint probability of independent variables is \n",
    "        computed and then how you may do this in log space.\n",
    "        '''\n",
    "        mu, std = self.forward(obs)\n",
    "        normal = torch.distributions.Normal(mu, std)\n",
    "        log_prob = normal.log_prob(action)\n",
    "        # Sum across action dimensions to get joint probability in log space\n",
    "        return log_prob.sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.2) Sample a trajectory\n",
    "\n",
    "You will now implement a function that samples a collection of trajectories. This function should be nearly identical to the `run_policy` function from HW7, with the difference that it will instead return a dictionary containing the data from the trajectory. \n",
    "\n",
    "I have provided below a `sample_trajectories` function that loops over calls to your `sample_trajectory` function to collect all the data and put it in the format we will need in subsequent functions.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "otter": {
     "tests": [
      "q1.2"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sample_trajectory_12(env, policy, seed=None):\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    observations, actions, rewards = [], [], []\n",
    "    steps = 0\n",
    "    # ***Your code here:***\n",
    "    # Note: you should discard the final observation (the one that is accompanied by a termination or truncation)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = policy.sample(obs_tensor).numpy().flatten()\n",
    "        \n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        obs = next_obs\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "\n",
    "    '''\n",
    "    observations: (steps, obs_dim)\n",
    "    actions: (steps, action_dim)\n",
    "    rewards: (steps,) \n",
    "    length: int = steps\n",
    "    '''\n",
    "    observations = np.array(observations, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.float32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    path = {'observations': torch.from_numpy(observations),\n",
    "            'actions': torch.from_numpy(actions),\n",
    "            'rewards': torch.from_numpy(rewards),\n",
    "            'length': steps}\n",
    "    return path\n",
    "\n",
    "# DO NOT MODIFY THE CODE BELOW\n",
    "def sample_trajectories_12(env, policy, min_batch_size, seed=None):\n",
    "    timesteps = 0\n",
    "    paths = []\n",
    "    itr = 0\n",
    "    while timesteps < min_batch_size:\n",
    "        with torch.no_grad():   # accelerate computation by turning off unnecessary gradients\n",
    "            path = sample_trajectory_12(env, policy, seed=seed+itr*1000)\n",
    "        itr += 1\n",
    "        paths.append(path)\n",
    "        timesteps += path['length']\n",
    "    return paths, timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.3) Computing the cumulative rewards\n",
    "\n",
    "You will now write code to compute $G_t=\\sum_{k=t}^T \\gamma^{k-t}R(S_t, A_t)$. \n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "otter": {
     "tests": [
      "q1.3"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sum_of_rewards_13(rewards, gamma):\n",
    "    ''' \n",
    "    rewards: list of torch tensors, each of which is the rewards for a single trajectory\n",
    "    gamma: float, the discount factor\n",
    "    return: torch tensor of shape (num_trajectories * num_steps, ), the reward-to-go for each time step, flattened\n",
    "    '''\n",
    "    all_rtgs = []\n",
    "    \n",
    "    for trajectory_rewards in rewards:\n",
    "        path_length = len(trajectory_rewards)\n",
    "        rtgs = torch.zeros_like(trajectory_rewards)\n",
    "        \n",
    "        # Compute reward-to-go for each timestep\n",
    "        for t in reversed(range(path_length)):\n",
    "            rtgs[t] = trajectory_rewards[t]\n",
    "            if t + 1 < path_length:\n",
    "                rtgs[t] += gamma * rtgs[t+1]\n",
    "                \n",
    "        all_rtgs.append(rtgs)\n",
    "    \n",
    "    # Concatenate all rewards-to-go\n",
    "    return torch.cat(all_rtgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.4) Loss function\n",
    "\n",
    "We derived the policy gradient to be:\n",
    "$$\n",
    "    \\nabla J(W) \\approx \\frac{1}{N\\cdot T} \\sum_{i=1}^{N} \\sum_{t=1}^{T} G_{t,i} \\frac{\\nabla\\pi_W(A_{t,i} \\mid S_{t,i})}{\\pi_W(A_{t,i} \\mid S_{t,i})}\\enspace.\n",
    "$$\n",
    "\n",
    "In order to leverage the autodiff capabilities of PyTorch, we need to write a *loss* function to do gradient **descent**. How should the gradient of the loss function relate to the gradient defined above? \n",
    "\n",
    "Write a loss function whose gradient is as desired for gradient descent.\n",
    "\n",
    "(Note: we made a minor change in our derivation to include a factor of $\\frac{1}{T}$. While this theoratically doesn't change anything---we can always scale the learning rate appropriately independently of what multiplicative factor we use---this choice makes implementation easier. Make sure that your loss function incorporates the appropriate scale in order for the autograder to work correctly, and for your learning rate to match the ones suggested in the assignment.)\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "otter": {
     "tests": [
      "q1.4"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def pg_loss_fn_14(policy, observations, actions, returns):\n",
    "    ''' \n",
    "    policy: GaussianPolicy_11 object\n",
    "    observations: Tensor of shape (num_trajectories * num_steps, obs_dim)\n",
    "    actions: Tensor of shape (num_trajectories * num_steps, action_dim)\n",
    "    returns: Tensor of shape (num_trajectories * num_steps, )\n",
    "    return: loss: Tensor of shape (1, ), a loss whose gradient can be used for gradient *descent*\n",
    "    '''\n",
    "    log_probs = policy.log_prob(observations, actions)\n",
    "\n",
    "    loss = -torch.mean(log_probs * returns)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.5) Putting it all together: PG training\n",
    "\n",
    "You will now complete the code below that implements PG training by putting together all the functions you have written so far. \n",
    "\n",
    "The function `train_PG_15` will implement the training loop, which internally calls the function `update_parameters_15`. The latter function takes one gradient descent step using the optimizer.\n",
    "\n",
    "In this question, you will implement one additional trick in `train_PG_15`: normalizing the returns. In particular, if the argument `normalize_returns=True`, your code should normalize the returns to have mean zero and standard deviation one. \n",
    "\n",
    "The parts of the code that you should modify are annotated with `*** 1.5 -- YOUR CODE HERE ***`.\n",
    "\n",
    "*Note: parts of this code are annotated with `*** 2.3 -- YOUR CODE HERE ***`. These parts of the code will not be tested in this question, but in later questions. You may leave those parts of the code unchanged until you reach the relevant problem.*\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "otter": {
     "tests": [
      "q1.5"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters_15(policy, observations, actions, returns, optimizer):\n",
    "    # *** 1.5 YOUR CODE HERE ***\n",
    "    loss = pg_loss_fn_14(policy, observations, actions, returns)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.detach().item()     # You may print this in your training function for debugging\n",
    "\n",
    "\n",
    "\n",
    "def train_PG_15(env_name, \n",
    "                hid_size=64, \n",
    "                num_layers=2, \n",
    "                use_baseline=False, \n",
    "                num_iterations=100, \n",
    "                batch_size=1000,\n",
    "                gamma=0.99,\n",
    "                normalize_returns=False,\n",
    "                learning_rate=1e-3,\n",
    "                seed=0):\n",
    "    # Make the gym environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Set the random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # *** 1.5 YOUR CODE HERE ***\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    policy = GaussianPolicy_11(obs_dim, action_dim, hid_size, num_layers)\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_baseline:\n",
    "        # *** 2.3 YOUR CODE HERE ***\n",
    "        nn_baseline = FCNN_11(obs_dim, 1, hid_size, num_layers)\n",
    "        optimizer_baseline = torch.optim.Adam(nn_baseline.parameters(), lr=1e-3)\n",
    "    \n",
    "    total_timesteps = 0\n",
    "    for iter in range(num_iterations):\n",
    "        print(f\"********** Iteration {iter} **********\")\n",
    "\n",
    "        # Sample trajectories\n",
    "        # *** 1.5 YOUR CODE HERE ***\n",
    "        paths, timesteps = sample_trajectories_12(env, policy, batch_size, seed=seed+iter*1000)\n",
    "        total_timesteps += timesteps\n",
    "\n",
    "        # Build tensors\n",
    "        observations = torch.cat([path['observations'] for path in paths], dim=0)\n",
    "        actions = torch.cat([path['actions'] for path in paths], dim=0)\n",
    "        rewards = torch.cat([path['rewards'] for path in paths], dim=0)\n",
    "        print(observations.shape, actions.shape, rewards.shape)\n",
    "        print(rewards)\n",
    "\n",
    "\n",
    "        # Print the average undiscounted return\n",
    "        undiscounted_return = rewards.sum() / len(paths)\n",
    "        print(f\"\\tAverage return: {undiscounted_return:.3f}\")\n",
    "\n",
    "        # Compute the reward-to-go\n",
    "        # *** 1.5 YOUR CODE HERE ***\n",
    "        rewards_to_go = sum_of_rewards_13(rewards, gamma)\n",
    "\n",
    "        if use_baseline:\n",
    "            # *** 2.3 YOUR CODE HERE ***\n",
    "            baseline = compute_baseline_21(nn_baseline, observations, rewards_to_go)\n",
    "            returns = rewards_to_go - baseline\n",
    "        else:\n",
    "            returns = rewards_to_go\n",
    "        if normalize_returns:\n",
    "            # *** 1.5 YOUR CODE HERE ***\n",
    "            # Normalize the returns\n",
    "            # Hint: Add a small value of 1e-5 to the denominator to avoid division by zero\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "\n",
    "        # Update the policy\n",
    "        # *** 1.5 YOUR CODE HERE ***\n",
    "        policy_loss = update_parameters_15(policy, observations, actions, returns, optimizer)\n",
    "        # print(f\"\\tPolicy loss: {loss:.5f}\")\n",
    "\n",
    "        if use_baseline:\n",
    "            # *** 2.3 YOUR CODE HERE ***\n",
    "            # Update the baseline\n",
    "            baseline_loss = update_baseline_parameters_23(nn_baseline, observations, rewards_to_go, optimizer_baseline)\n",
    "            # print(f\"\\tBaseline loss: {baseline_loss:.5f}\")\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.6) Experiments\n",
    "\n",
    "Run experiments using `env_name = \"InvertedPendulum-v5\"`. Use the following arguments to your `train_PG` function:\n",
    "- `hid_size`: 64\n",
    "- `num_layers`: 2\n",
    "- `num_iterations`: 100\n",
    "- `batch_size`: ?\n",
    "- `gamma`: 0.99\n",
    "- `normalize_returns`: ?\n",
    "- `learning_rate`: ?\n",
    "- `seed`: 0\n",
    "\n",
    "Find values for the `batch_size`, `normalize_returns`, and `learning_rate` such that the agent achieves the optimal sum of rewards for this environment (1000) in less than 100 iterations. For your choice of hyperparameters, your code should run in just a few seconds --- this limits the `batch_size` that you can choose, or the autograder will time out. Some suggested values to try are included below:\n",
    "- `batch_size in [10, 100, 1000, 3000]`\n",
    "- `learning_rate in [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2]`\n",
    "- `normalize_returns in [True, False]`\n",
    "\n",
    "Report your chosen hyperparameters below.\n",
    "\n",
    "The code cell below includes code to visualize your learned policy, which you can use throughout this assignment to observe the behaviors that your learned policy achieves.\n",
    "\n",
    "_Points:_ 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def visualize_policy(env_name, policy, num_episodes=1):\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.sample(torch.from_numpy(obs.astype(np.float32)))\n",
    "            action = action.detach().numpy()\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            env.render()\n",
    "            done = terminated or truncated\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "otter": {
     "tests": [
      "q1.6"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size_16 = 1000\n",
    "normalize_returns_16 = True\n",
    "learning_rate_16 = 3e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) REINFORCE with baseline\n",
    "\n",
    "We will now implement a value function as a state-dependent neural network baseline. This will allow us to reduce the variance of the gradient computation and hopefully learn to solve significantly harder problems. \n",
    "\n",
    "You will implement the vanilla version, which simply uses $\\mathcal{L}(\\theta) = \\frac{1}{NT}\\sum_{i=1}^N\\sum_{t=1}^T(G_t - V^\\pi_\\theta(s_t))^2$, where $G_t$ is the reward-to-go measured directly from the trajectories.\n",
    "\n",
    "For this, it will be useful to apply the following normalization trick:\n",
    "- During training of $V^\\pi_\\theta$, instead of using the reward-to-go $G_t$ as the target labels, you will first normalize the reward-to-go to have mean zero and standard deviation 1. As we have seen in HW7, this is generally a good idea for NN training. But it is particularly helpful in this case where the scale of the reward-to-go may change over time --- we certainly hope that reward goes up as the agent learns!\n",
    "- When applying the baseline to compute the policy gradient, you will compute the baseline values and scale them so that their mean and standard deviation match those of the reward-to-go in the current batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.1) Computing the baseline\n",
    "\n",
    "Your first task is to compute, given a NN baseline $V^\\pi_\\theta$, the baseline value $V_\\theta^\\pi(s_t)$. Make sure that the baseline value is appropriately scaled to have the same mean and standard deviation as the reward-to-go.\n",
    "\n",
    "As usual, you should take care not to divide by zero by adding 1e-5 to any denominator that could go to zero.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "otter": {
     "tests": [
      "q2.1"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_baseline_21(nn_baseline, observations, rewards_to_go):\n",
    "    ''' \n",
    "    nn_baseline: FCNN_11 object\n",
    "    observations: Tensor of shape (num_trajectories * num_steps, obs_dim)\n",
    "    rewards_to_go: Tensor of shape (num_trajectories * num_steps,)\n",
    "    return: Tensor of shape (num_trajectories * num_steps,) the baseline values computed from nn_baseline, with the same mean and standard dev as returns\n",
    "    '''\n",
    "    with torch.no_grad():   # We don't backgprop through the baseline when computing PGs\n",
    "        baseline_values = nn_baseline(observations).squeeze()\n",
    "\n",
    "        rtg_mean = rewards_to_go.mean()\n",
    "        rtg_std = rewards_to_go.std()\n",
    "\n",
    "        baseline_mean = baseline_values.mean()\n",
    "        baseline_std = baseline_values.std() + 1e-5\n",
    "\n",
    "        baseline = (baseline_values - baseline_mean) / baseline_std * rtg_std + rtg_mean\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.2) Computing the baseline loss\n",
    "\n",
    "You will now compute the loss function to train the baseline on. This will be the mean squared error of the predictions of the baseline compared to the reward-to-go. To simplify the training, you will normalize the reward-to-go targets to have zero mean and standard deviation one.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "otter": {
     "tests": [
      "q2.2"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def baseline_loss_22(nn_baseline, observations, rewards_to_go):\n",
    "    ''' \n",
    "    nn_baseline: FCNN_11 object\n",
    "    observations: Tensor of shape (num_trajectories * num_steps, obs_dim)\n",
    "    rewards_to_go: Tensor of shape (num_trajectories * num_steps,)\n",
    "    return: singleton Tensor of shape ([]), the mean square error\n",
    "\n",
    "    Hint: Think about the shapes of the output of nn_baseline and the rewards_to_go, and \n",
    "    how this would work if you use broadcasting vs manual reshaping.\n",
    "    '''\n",
    "    # Normalize ~rewards_to_go to have mean 0 and std 1\n",
    "    rtg_mean = rewards_to_go.mean()\n",
    "    rtg_std = rewards_to_go.std() + 1e-5 \n",
    "    normalized_rtg = (rewards_to_go - rtg_mean) / rtg_std\n",
    "    \n",
    "    # Get baseline predictions\n",
    "    baseline_predictions = nn_baseline(observations).squeeze()\n",
    "    \n",
    "    # Compute mean squared error\n",
    "    mse = torch.mean((baseline_predictions - normalized_rtg) ** 2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.3) Putting it all together: PG training with baseline\n",
    "\n",
    "You will now complete the code below and in the `train_PG_15` function (from Problem 1.5) to incorprate the baseline training into the PG loop. Complete any parts annotated with `*** 2.3 YOUR CODE HERE ***`. \n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "otter": {
     "tests": [
      "q2.3"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def update_baseline_parameters_23(nn_baseline, observations, rewards_to_go, optimizer):\n",
    "    # *** 2.3 YOUR CODE HERE ***\n",
    "    loss = baseline_loss_22(nn_baseline, observations, rewards_to_go)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.detach().item()     # You may print this in your training function for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.4) Experiments with more complex tasks\n",
    "\n",
    "**Note:** The following tasks will take quite a bit of time to train. Please start early!\n",
    "\n",
    "Run experiments using `env_name = \"HalfCheetah-v5\"`. Use the following arguments to your `train_PG` function:\n",
    "- `hid_size`: 32\n",
    "- `num_layers`: 2\n",
    "- `use_baseline`: ?\n",
    "- `num_iterations`: 100\n",
    "- `batch_size`: ?\n",
    "- `gamma`: 0.95\n",
    "- `normalize_returns`: True\n",
    "- `learning_rate`: ?\n",
    "- `seed`: 0\n",
    "\n",
    "Find values for the `use_baseline`, `batch_size`, and `learning_rate` such that the agent achieves the highest sum of rewards that you can for this environment in less than 100 iterations. Some suggested values to try are included below:\n",
    "- `batch_size in [10000, 30000, 50000]`\n",
    "- `learning_rate in [1e-3, 3e-3, 1e-2, 3e-2, 1e-3]`\n",
    "- `use_baseline in [True, False]`\n",
    "\n",
    "You should be able to obtain sum of rewards above 1800 with an appropriate choice of hyperparameters.\n",
    "\n",
    "Unlike Problem 1.6, this time you should train your policy in a separate notebook and save it as `trained_cheetah_policy.pt`. Because PG training is quite noisy, it is unlikely that the policy at iteration 100 is the best performing one, so you may want to add the following lines to your `train_PG` function in your other notebook:\n",
    "\n",
    "```\n",
    "...\n",
    "total_timesteps = 0\n",
    "highest_returns = -np.inf\n",
    "for iter in range(num_iterations):\n",
    "    ... \n",
    "    undiscounted_return = rewards.sum() / len(paths)\n",
    "    if undiscounted_return > highest_returns:\n",
    "        torch.save(policy.state_dict(), 'trained_cheetah_policy.pt')\n",
    "        highest_returns = undiscounted_return\n",
    "    ...\n",
    "```\n",
    "\n",
    "_Points:_ 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 **********\n",
      "torch.Size([20000, 17]) torch.Size([20000, 6]) torch.Size([20000])\n",
      "tensor([-0.6637, -1.7120, -0.0399,  ..., -0.2590, -0.6562,  0.2577])\n",
      "\tAverage return: -685.350\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.5e-2\u001b[39m  \n\u001b[0;32m     10\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 13\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_PG_15\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhid_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_baseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_baseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 63\u001b[0m, in \u001b[0;36mtrain_PG_15\u001b[1;34m(env_name, hid_size, num_layers, use_baseline, num_iterations, batch_size, gamma, normalize_returns, learning_rate, seed)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAverage return: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mundiscounted_return\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Compute the reward-to-go\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# *** 1.5 YOUR CODE HERE ***\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m rewards_to_go \u001b[38;5;241m=\u001b[39m \u001b[43msum_of_rewards_13\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_baseline:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# *** 2.3 YOUR CODE HERE ***\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     baseline \u001b[38;5;241m=\u001b[39m compute_baseline_21(nn_baseline, observations, rewards_to_go)\n",
      "Cell \u001b[1;32mIn[31], line 10\u001b[0m, in \u001b[0;36msum_of_rewards_13\u001b[1;34m(rewards, gamma)\u001b[0m\n\u001b[0;32m      7\u001b[0m all_rtgs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trajectory_rewards \u001b[38;5;129;01min\u001b[39;00m rewards:\n\u001b[1;32m---> 10\u001b[0m     path_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrajectory_rewards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     rtgs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(trajectory_rewards)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Compute reward-to-go for each timestep\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\reach\\miniconda3\\envs\\robotic\\lib\\site-packages\\torch\\_tensor.py:1132\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen() of a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m   1134\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing len to get tensor shape might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended usage would be tensor.shape[0]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1140\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1141\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of a 0-d tensor"
     ]
    }
   ],
   "source": [
    "env_name = \"HalfCheetah-v5\"\n",
    "hid_size = 32\n",
    "num_layers = 2\n",
    "use_baseline = True  \n",
    "num_iterations = 1\n",
    "batch_size = 20000 \n",
    "gamma = 0.95\n",
    "normalize_returns = True\n",
    "learning_rate = 1.5e-2  \n",
    "seed = 0\n",
    "\n",
    "\n",
    "policy = train_PG_15(\n",
    "    env_name=env_name,\n",
    "    hid_size=hid_size,\n",
    "    num_layers=num_layers,\n",
    "    use_baseline=use_baseline,\n",
    "    num_iterations=num_iterations,\n",
    "    batch_size=batch_size,\n",
    "    gamma=gamma,\n",
    "    normalize_returns=normalize_returns,\n",
    "    learning_rate=learning_rate,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "q2.4"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Nothing to do in this code cell. Please run your training and saving code in a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Fill out the answers to all questions. Submit a zip file containing hw8.ipynb with your answers and the `trained_cheetah_policy.pt` file you saved to the HW8 assignment on Gradescope. You are free to resubmit as many times as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running your submission against local test cases...\n",
      "\n",
      "\n",
      "Your submission received the following results when run against available test cases:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <p>\n",
       "                        Your submission has been exported. Click\n",
       "                        <a href=\"hw8_2025_04_21T19_53_05_804713.zip\" download=\"hw8_2025_04_21T19_53_05_804713.zip\" target=\"_blank\">here</a> to download\n",
       "                        the zip file.\n",
       "                    </p>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "# grader.export(pdf=False, run_tests=True, files=['trained_cheetah_policy.pt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "hw8",
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.3": {
     "name": "q1.3",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.4": {
     "name": "q1.4",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.5": {
     "name": "q1.5",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.6": {
     "name": "q1.6",
     "points": 15,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": 5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.4": {
     "name": "q2.4",
     "points": 20,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
