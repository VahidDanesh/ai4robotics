{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw7.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILL IN YOUR NAME AND THE NAME OF YOUR PEER (IF ANY) BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Vahid Danesh\n",
    "\n",
    "**Peer**: Claude Sonnet 3.7\n",
    "\n",
    "## Collaboration policy\n",
    "Students are responsible for writing their own quizzes, assignments, and exams. For homework assignments, students are welcome (and encouraged) to discuss problems with one peer, **but each student must write their own assignment wrtieup and code individually**. The peer must be listed at the top of the writeup for each assignment. *Note: I will treat AI assistants as peers. That is, students are welcome to discuss problems with an AI assistant, but it is considered cheating to directly obtain an answer by querying the assistant. Please credit any AI assistant that you use.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7 -- Behavior cloning (100 pts)\n",
    "\n",
    "**Due:** Tuesday, April 8th, 2025 at 11:59 pm\n",
    "\n",
    "This homework builds on the material in the slides and MIT Lecture notes Chapter 8 (on Brightspace).\n",
    "\n",
    "We will use Jupyter/Colab notebooks throughout the semester for writing code and generating assignment outputs.\n",
    "\n",
    "**This homework will be unlike prior homeworks. It will be _entirely_ implementation-based. Some questions will be assessed by running your code, while others will require you to upload trained neural nets, which the grader will evaluate.**\n",
    "\n",
    "## 1) Neural net implementation\n",
    "\n",
    "Our first step will be to create a neural net implementation, including training code. This part will be agnostic to the robotics problem setting: we will simply train a neural net on some arbitrary X, Y dataset.\n",
    "\n",
    "In this homework, we'll use PyTorch, a Python framework for implementing and training neural networks. PyTorch is currently one of the most popular frameworks within the machine learning community, so it's worth becoming familiar with it.\n",
    "\n",
    "In particular, PyTorch uses the **Tensor** data type to encode all the objects it works on. In our usage, a tensor is the generalization of the idea of an array (which is typically 2D) to a structure that can have arbitrarily many dimensions. We will use tensors to represent data sets, batches of data, weights, activations, etc. They are very versatile but can be a little bit difficult to conceptualize especially when you're starting out. You'll get the hang of it! I recommend looking at this very good [primer on PyTorch tensors](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html).\n",
    "\n",
    "To run locally, you may simply run `pip install torch`; PyTorch is already installed in Google Colab. \n",
    "\n",
    "PyTorch creates networks in a modular fashion, by chaining different [torch.nn.Modules](https://pytorch.org/docs/stable/nn.html) together. Some helpful modules for this assignment include:\n",
    "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) -- implements (the pre-activations for) a fully-connected layer, what we call $W^\\top x + b$.\n",
    "- [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) -- implements ReLU activation.\n",
    "- [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) -- implements element-wise Mean Squared Error.\n",
    "\n",
    "\n",
    "\n",
    "For an example of how all these pieces fit together to create a network, take a look at this [quickstart tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) from the PyTorch website.\n",
    "\n",
    "Throughout this question, you can use the dataset in `dummy_data.pt`, included with this assignment. You may load this file using `data = torch.load(\"dummy_data.pt\")`, which will return dictionary of the form: `{'inputs_train': X_train, 'outputs_train': Y_train, 'inputs_test': X_test, 'outputs_test': Y_test}`. Inspect the X and Y to determine what the input-output dimensions to your network should be.\n",
    "- You should interpret the *_train as being the inputs/outputs that you will use to execute gradient descent on your network and the *_test as being the inputs/outputs that you will evaluate your model on to see if it is learning something meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.1) Fully-connected network\n",
    "\n",
    "Your first task is to create a class that constructs a fully-connected network. The class constructor will take as input the following arguments:\n",
    "- `input_dim`: the number of features in the input\n",
    "- `output_dim`: the number of outputs that the network should produce\n",
    "- `hid_size`: the number of neurons in the hidden layers\n",
    "- `num_layers`: the number of hidden layers in your network. (*Note: in NN-speak, the \"input\" layer is the actual features, the \"output\" layer is the final set of nodes, and the \"hidden\" layers are all the ones in between.*)\n",
    "\n",
    "Your network should be able to consume a batch of inputs of shape `(n, input_dim)` (where `n` is an arbitrary integer) and produce a batch of outputs of shape `(n, output_dim)` after going through `num_layers` hidden layers with **ReLU** activation. The output layer should not have any activation.\n",
    "\n",
    "*Note: if you choose to store your hidden layers in a list, you should use [`nn.ModuleList`](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList), which operates just like a standard Python list, but has additional attributes that enable Pytorch to use its elements during backpropagation.*\n",
    "\n",
    "_Points:_ 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "otter": {
     "tests": [
      "q1.1"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FCNN_11(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hid_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_dim, hid_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hid_size, hid_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hid_size, output_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.2) Data normalization\n",
    "\n",
    "Neural net training via backpropagation on data that is normalized to be within some relatively constrained range. One common data normalization technique is to *standardize* the data to have zero mean and standard deviation one. This is achieved by computing:\n",
    "$$X_\\text{norm} = \\frac{X - \\mu}{\\sigma}\\enspace.$$\n",
    "\n",
    "\n",
    "There are three key points to consider when normalizing data:\n",
    "- Both inputs and outputs should be normalized\n",
    "- Normalization should be done for each dimension in the input/output separately (e.g., if one feature dimension is the robot's joint 0 position, then joint 0 should be normalized so that the mean of joint 0 positions is 0)\n",
    "- Normalization should be based on *training* data statistics, and never on *test* data statistics. This is because we want to use the same constants to scale training and test data (and test data is not available during training)\n",
    "\n",
    "Write a Python class to normalize data via standardization. Your class constructor should receive a torch tensor `X` as input and store the relevant statistics from `X` to use for standardization. \n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "otter": {
     "tests": [
      "q1.2"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class Normalizer_12:\n",
    "    def __init__(self, X):\n",
    "        ''' \n",
    "        Add a small constant 1e-5 to the standard deviation to avoid division by zero\n",
    "        '''\n",
    "        self.mean = torch.mean(X, dim=0)\n",
    "        self.std = torch.std(X, dim=0) + 1e-5\n",
    "    \n",
    "    def normalize(self, X):\n",
    "        '''\n",
    "        Given a tensor X, return the normalized tensor using the mean and std\n",
    "        '''\n",
    "        return (X - self.mean) / self.std\n",
    "    \n",
    "    def denormalize(self, X_normalized):\n",
    "        ''''\n",
    "        Given a normalized tensor X_normalized, return the denormalized tensor using the mean and std\n",
    "        '''\n",
    "        return X_normalized * self.std + self.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.3) Training\n",
    "\n",
    "Now you will write a function to train the network to minimize the MSE loss on a given training data. \n",
    "\n",
    "It will be helpful to leverage the following two utilities:\n",
    "- [`torch.utils.data.TorchDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) -- takes multiple tensors as input and constructs a Dataset object to use as input to a DataLoader (see below)\n",
    "- [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) -- takes a Dataset as input and produces an object that can be iterated over. Using `shuffle=True` makes each loop over the DataLoader produce a different ordering over the data.\n",
    "\n",
    "Read the documentation and examples to understand how to use these two functions.\n",
    "\n",
    "Write the train function, which takes the following arguments:\n",
    "- `X`: the *unnormalized* training set inputs\n",
    "- `Y`: the *unnormalized* training set outputs\n",
    "- `net`: the neural net to train\n",
    "- `num_epochs`: the number of epochs (loops over the whole dataset) to execute\n",
    "- `batchsize`: the size of minibatches to train on (passed as input to `DataLoader`)\n",
    "\n",
    "You should normalize `X` and `Y`, then create your Dataset and DataLoader, and loop for `num_epochs` many rounds of training, taking gradient steps to minimize the MSE loss.\n",
    "\n",
    "The return value of your function should be the `X_normalizer` and `Y_normalizer` (the network itself is trained in-place, and so you do not need to return it).\n",
    "\n",
    "_Points:_ 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "otter": {
     "tests": [
      "q1.3"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "def train_13(X, Y, net, num_epochs, batchsize):\n",
    "    X_normalizer = Normalizer_12(X)\n",
    "    X = X_normalizer.normalize(X)\n",
    "    Y_normalizer = Normalizer_12(Y)\n",
    "    Y = Y_normalizer.normalize(Y)\n",
    "    \n",
    "    # Create a DataLoader for the training data\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_Y in dataloader:\n",
    "            # Zero the gradients: this is a weird PyTorch thing needed to avoid summing gradients over multiple rounds\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            Yhat = net(batch_X)\n",
    "            loss = criterion(Yhat, batch_Y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return X_normalizer, Y_normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Behavior cloning\n",
    "\n",
    "In this second half of the homework, we will train behavior policies for controlling a robot. For this, we will fit together two APIs:\n",
    "- [`minari`](https://github.com/Farama-Foundation/Minari) -- A library that standardizes behavior cloning (and, more generally, offline RL) data formats and contains a collection of simulated demonstration datasets in MuJoCo environments\n",
    "- [`gymnasium`](https://github.com/Farama-Foundation/Gymnasium) -- A libary that standardizes interaction with RL-like environments\n",
    "\n",
    "You can pip install everything you need for this part of the assignment with:\n",
    "```\n",
    "pip install mujoco==3.2.3\n",
    "pip install \"minari[hf,hdf5]\"\n",
    "pip install gymnasium[mujoco]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.1) Creating dataset and environment\n",
    "\n",
    "In this assignment, we will be working with the [`mujoco/pusher/expert-v0`](https://minari.farama.org/main/datasets/mujoco/pusher/expert-v0/) dataset.\n",
    "\n",
    "In this question, you will load the dataset, construct X and Y tensors to pass into your training code, and construct a simulation environment (which we will later use for running our policies). \n",
    "- `dataset = minari.load_dataset(<name>, download=True)` gives a dataset object. Dataset objects can be iterated via `for episode in dataset`.\n",
    "    - Episode objects have a `.observations` array and a `.actions` array\n",
    "- `env = dataset.recover_environment()` gives the desired simulation environment. During debugging, you may wish to pass the argument `render_mode=human` to observe the behavior of your policies. It is likely that this will cause issues on Gradescope, so be sure to turn rendering off (by removing the argument) from the env creation\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "otter": {
     "tests": [
      "q2.1"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import minari\n",
    "import gymnasium\n",
    "\n",
    "def minari_data_21(dataset_name):\n",
    "    '''\n",
    "    Given the name of a minari dataset name, return:\n",
    "    - X: an n x d tensor of observations (to use as input to a dataloader)\n",
    "    - Y: an n x m tensor of actions (to use as input to a dataloader) \n",
    "    - the simulation environment for interaction\n",
    "    Note: X and Y should \"get rid\" of the episode structure and just return\n",
    "    the observations and actions in a single array.\n",
    "    '''\n",
    "    dataset = minari.load_dataset(dataset_name, download=True)\n",
    "    env = dataset.recover_environment(render_mode=\"human\")\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    # *** Loop over the dataset and extract all observations and actions to construct the X,Y tensors ***\n",
    "    for episode in dataset:\n",
    "        observations.extend(episode.observations[:-1])\n",
    "        actions.extend(episode.actions)\n",
    "\n",
    "    X = torch.tensor(np.array(observations), dtype=torch.float32)\n",
    "    Y = torch.tensor(np.array(actions), dtype=torch.float32)\n",
    "    print(\"Data Loaded.\")\n",
    "    return X, Y, env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.2) Running a neural net policy\n",
    "\n",
    "You will now write a function that takes as input a network and a simulation environment and runs the policy on the simulator. \n",
    "\n",
    "The `env` API has the following methods:\n",
    "- `env.reset()`: Sets the environment to a (possibly random) initial state. Returns:\n",
    "    - `obs`: an observation of the initial state\n",
    "    - `info`: a dictionary containing information about the environment (we will not use this)\n",
    "- `env.step(action)`: Executes the action on the simulator. Returns:\n",
    "    - `obs`: an observation of the resulting state after running the action\n",
    "    - `rew`: the reward obtained by the agent for executing the action in that state\n",
    "    - `terminated`: whether the episode terminated (e.g., the agent died)\n",
    "    - `truncated`: whether the episode terminated due to some fixed timeout\n",
    "    - `info`: unused\n",
    "\n",
    "Your function should execute one full episode that passes each new state as input through a network and executed the action predicted by the network. Use the `terminated` and `truncated` signals to determine when the episode has terminated.\n",
    "\n",
    "Return the sum of rewards accumulated throughout the episode.\n",
    "\n",
    "*Hint: think carefully about how to use your normalizers in this function.*\n",
    "\n",
    "_Points:_ 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "otter": {
     "tests": [
      "q2.2"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def run_policy_22(net, env, X_normalizer, Y_normalizer, seed=None):\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not (terminated or truncated):\n",
    "        # Normalize observation\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        obs_norm = X_normalizer.normalize(obs_tensor)\n",
    "        \n",
    "        # Get action from network\n",
    "        with torch.no_grad():\n",
    "            action_norm = net(obs_norm)\n",
    "            action = Y_normalizer.denormalize(action_norm).squeeze(0).numpy()\n",
    "        \n",
    "        # Step environment\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.3) Training a good behavior cloning policy\n",
    "\n",
    "You will now (on your own) try various combinations of number of hidden layers, layer sizes, batchsize, and number of epochs. Because this takes a considerable amount of time, the autograder will not run this part of the code. Instead, once you find a good combination of values, you will train a network and save it using the following code cell. You can either:\n",
    "- run the training on this same notebook. In that case, you'll need to remove it (including the saving code) from the notebook before submitting (otherwise, the autograder will very likely time out); or\n",
    "- run the training on another notebook. In that case, you'll need to copy the code below to your other notebook to save your network.\n",
    "\n",
    "*Hint 1: I recommend that you run this on Google Colab, to leverage their free GPUs. For that, you'll need to click on the little triangle in the upper right and hit \"change runtime type\" and choose \"T4 GPU\". Then, you'll need to add `net = net.to('cuda')` and `X = X.to(cuda)` and `Y = Y.to(cuda)` so that your tensors and network are all on GPU.*\n",
    "\n",
    "*Hint 2: consider the following when choosing your hyperparameters:*\n",
    "- Hidden layers: more than a handful is probably too many\n",
    "- Layer sizes: more than a few thousand is probably too big\n",
    "- Batchsize: I'll let you figure this one out, but for speed, it's best to pick powers of 2 (actually, powers of 8 is even better)\n",
    "- epochs: more than a few hundred is probably too many\n",
    "\n",
    "*Hint 3: use the `render_mode=human` argument to visualize your policy with your `run_policy` function. A good policy should (almost always) succeed in pushing the block.*\n",
    "\n",
    "Your score will be based on how high a reward your policy obtains on my tests.\n",
    "\n",
    "_Points:_ 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Save trained net\\nsave_dict = {\\n    'net': net.state_dict(),\\n    'hid_size': ..., # The size of your hidden layers\\n    'num_layers': ..., # The number of hidden layers\\n    'X_mean': ..., # The mean of the observations\\n    'X_std': ..., # The std of the observations\\n    'Y_mean': ..., # The mean of the actions\\n    'Y_std': ..., # The std of the actions\\n}\\ntorch.save(save_dict, 'trained_pusher_policy.pt')'\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Save trained net\n",
    "save_dict = {\n",
    "    'net': net.state_dict(),\n",
    "    'hid_size': ..., # The size of your hidden layers\n",
    "    'num_layers': ..., # The number of hidden layers\n",
    "    'X_mean': ..., # The mean of the observations\n",
    "    'X_std': ..., # The std of the observations\n",
    "    'Y_mean': ..., # The mean of the actions\n",
    "    'Y_std': ..., # The std of the actions\n",
    "}\n",
    "torch.save(save_dict, 'trained_pusher_policy.pt')'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Fill out the answers to all questions. Submit a zip file containing hw7.ipynb with your answers and the `trained_pusher_policy.pt` file you saved to the HW7 assignment on Gradescope. You are free to resubmit as many times as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running your submission against local test cases...\n",
      "\n",
      "\n",
      "Your submission received the following results when run against available test cases:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <p>\n",
       "                        Your submission has been exported. Click\n",
       "                        <a href=\"hw7_2025_04_09T21_22_46_241024.zip\" download=\"hw7_2025_04_09T21_22_46_241024.zip\" target=\"_blank\">here</a> to download\n",
       "                        the zip file.\n",
       "                    </p>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True, files=['trained_pusher_policy.pt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "hw7",
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 15,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.3": {
     "name": "q1.3",
     "points": 20,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 20,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 25,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
